
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>wget 使用技巧 - Ocean's Blog</title>
	<meta name="author" content="Ocean">

	
	<meta name="description" content="wget 是一个命令行的下载工具。对于我们这些 Linux 用户来说，几乎每天都在使用它。下面为大家介绍几个有用的 wget 小技巧，可以让你更加高效而灵活的使用 wget。 $ wget -r -np -nd http://example.com/packages/ 这条命令可以下载 http &hellip;">
	
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Ocean's Blog" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script async="true" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	
</head>


<body>
	<header id="header" class="inner"><h1><a href="/">Ocean's Blog</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/">主页</a></li>
	<li><a href="/blog/archives">存档</a></li>
	<li><a href="/blog/categories">分类</a></li>
	<li><a href="/blog/about">关于</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/">主页</a></li>
	<li><a href="/blog/archives">存档</a></li>
	<li><a href="/blog/categories">分类</a></li>
	<li><a href="/blog/about">关于</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="http://google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:blog.nwy.me">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
    
	</div>
	<form class="search" action="http://google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:blog.nwy.me">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner"><article class="post">
	<h2 class="title">Wget 使用技巧</h2>
	<div class="entry-content"><p><a href="http://www.gnu.org/software/wget/">wget</a> 是一个命令行的下载工具。对于我们这些 Linux 用户来说，几乎每天都在使用它。下面为大家介绍几个有用的 wget 小技巧，可以让你更加高效而灵活的使用 wget。</p>

<ul>
<li><code>$ wget -r -np -nd http://example.com/packages/</code></li>
</ul>


<p>这条命令可以下载 http://example.com 网站上 packages 目录中的所有文件。其中，<code>-np</code> 的作用是不遍历父目录，<code>-nd</code> 表示不在本机重新创建目录结构。</p>

<ul>
<li><code>$ wget -r -np -nd --accept=iso http://example.com/centos-5/i386/</code></li>
</ul>


<p>与上一条命令相似，但多加了一个 <code>--accept=iso</code> 选项，这指示 wget 仅下载 i386 目录中所有扩展名为 iso 的文件。你也可以指定多个扩展名，只需用逗号分隔即可。</p>

<ul>
<li><code>$ wget -i filename.txt</code></li>
</ul>


<p>此命令常用于批量下载的情形，把所有需要下载文件的地址放到 filename.txt 中，然后 wget 就会自动为你下载所有文件了。</p>

<ul>
<li><code>$ wget -c http://example.com/really-big-file.iso</code></li>
</ul>


<p>这里所指定的 <code>-c</code> 选项的作用为断点续传。</p>

<ul>
<li><code>$ wget -m -k (-H) http://www.example.com/</code></li>
</ul>


<p>该命令可用来镜像一个网站，wget 将对链接进行转换。如果网站中的图像是放在另外的站点，那么可以使用 <code>-H</code> 选项。</p>

<p>以上文字Other自<a href="http://linuxtoy.org/archives/wget-tips.html">这里</a>。</p>

<p>补充整理：</p>

<p>wget的主要特点包括：</p>

<ul>
<li>支持递归下载</li>
    <li>恰当的转换页面中的链接</li>
    <li>生成可在本地浏览的页面镜像</li>
    <li>支持代理服务器</li>
</ul>


<p>在其之上的图形界面应用程序有：<a title="GNOME" href="http://zh.wikipedia.org/wiki/GNOME">GNOME</a>下面的<strong>gwget</strong>。Windows系统下面的<strong>wGetGUI。</strong>多线程方式,推荐用axel，多线程抓站还可以用 pavuk.国内有很多下载地址是加了跳转的 ,不能直接wget的，可以在网址上加单引号&#8221;这样就可以了。</p>

<h4>附录：<a href="http://pengjiayou.com/blog/wget-use-skills-daquan" target="_blank">wget 使用技巧大全</a>
</h4>


<p>Wget 的使用</p>

<p>1）支持断点下传功能（2）同时支持FTP和HTTP下载方式（3）支持代理服务器（4）设置方便简单；5）程序小，完全免费；</p>

<p>命令格式：</p>

<p>wget [参数列表] [目标软件、网页的网址]</p>

<p>1、启动类参数</p>

<p>这一类参数主要提供软件的一些基本信息；</p>

<p>-V,&#8211;version 显示软件版本号然后退出；
-h,&#8211;help显示软件帮助信息；
-e,&#8211;execute=COMMAND 执行一个 “.wgetrc”命令</p>

<p>以上每一个功能有长短两个参数，长短功能一样，都可以使用。需要注意的是，这里的-e参数是执行一个.wgettrc的命令，.wgettrc命令其实是一个参数列表，直接将软件需要的参数写在一起就可以了。</p>

<p>2、文件处理参数</p>

<p>这类参数定义软件log文件的输出方式等；</p>

<p>-o,&#8211;output-file=FILE 将软件输出信息保存到文件；
-a,&#8211;append-output=FILE将软件输出信息追加到文件；
-d,&#8211;debug显示输出信息；
-q,&#8211;quiet 不显示输出信息；
-i,&#8211;input-file=FILE 从文件中取得URL；</p>

<p>以上参数对于攻击者比较有用，我们来看看具体使用；</p>

<p>例1：下载192.168.1.168首页并且显示下载信息
wget -d http://192.168.1.168</p>

<p>例2：下载192.168.1.168首页并且不显示任何信息
wget -q http://192.168.1.168</p>

<p>例3：下载filelist.txt中所包含的链接的所有文件
wget -i filelist.txt</p>

<p>wget -np -m -l5  http://jpstone.bokee.com //不下载本站所链接的其它站点内容，5级目录结构
3、下载参数</p>

<p>下载参数定义下载重复次数、保存文件名等；</p>

<p>-t,&#8211;tries=NUMBER 是否下载次数（0表示无穷次）
-O &#8211;output-document=FILE下载文件保存为别的文件名
-nc, &#8211;no-clobber 不要覆盖已经存在的文件
-N,&#8211;timestamping只下载比本地新的文件
-T,&#8211;timeout=SECONDS 设置超时时间
-Y,&#8211;proxy=on/off 关闭代理</p>

<p>例：下载192.168.1.168的首页并将下载过程中的的输入信息保存到test.htm文件中
wget -o test.htm http://192.168.1.168</p>

<p>4、目录参数</p>

<p>目录参数主要设置下载文件保存目录与原来文件（服务器文件）的目录对应关系；</p>

<p>-nd  &#8211;no-directories 不建立目录
-x,&#8211;force-directories 强制建立目录
可能现在我们对这里的目录还不是很了解，我们来看一个举例</p>

<p>例：下载192.168.1.168的首页，并且保持网站结构
wget -x http://192.168.1.168</p>

<p>5、HTTP参数</p>

<p>HTTP参数设置一些与HTTP下载有关的属性；</p>

<p>&#8211;http-user=USER设置HTTP用户
&#8211;http-passwd=PASS设置HTTP密码
&#8211;proxy-user=USER设置代理用户
&#8211;proxy-passwd=PASS设置代理密码</p>

<p>以上参数主要设置HTTP和代理的用户、密码；</p>

<p>6、递归参数设置</p>

<p>在下载一个网站或者网站的一个目录的时候，我们需要知道的下载的层次，这些参数就可以设置；
-r,&#8211;recursive 下载整个网站、目录（小心使用）
-l,&#8211;level=NUMBER 下载层次</p>

<p>例：下载整个网站
wget -r http://192.168.1.168</p>

<p>7、递归允许与拒绝选项参数</p>

<p>下载一个网站的时候，为了尽量快，有些文件可以选择下载，比如图片和声音，在这里可以设置；</p>

<p>-A,&#8211;accept=LIST 可以接受的文件类型
-R,&#8211;reject=LIST拒绝接受的文件类型
-D,&#8211;domains=LIST可以接受的域名
&#8211;exclude-domains=LIST拒绝的域名
-L,&#8211;relative 下载关联链接
&#8211;follow-ftp 只下载FTP链接
-H,&#8211;span-hosts 可以下载外面的主机
-I,&#8211;include-directories=LIST允许的目录
-X,&#8211;exclude-directories=LIST 拒绝的目录</p>

<p>如何设定wget所使用的代理服务器
wget可以使用用户设置文件&#8221;.wgetrc&#8221;来读取很多设置，我们这里主要利用这个文件来是
设置代理服务器。使用者用什么用户登录，那么什么用户主目录下的&#8221;.wgetrc&#8221;文件就起
作用。例如，&#8221;root&#8221;用户如果想使用&#8221;.wgetrc&#8221;来设置代理服务器，&#8221;/root/.wgetrc&#8221;就起
作用，下面给出一个&#8221;.wgetrc&#8221;文件的内容，读者可以参照这个例子来编写自己的&#8221;wgetrc&#8221;文件：
http-proxy = 111.111.111.111:8080
ftp-proxy = 111.111.111.111:8080
这两行的含义是，代理服务器IP地址为：111.111.111.111，端口号为：80。第一行指定
HTTP协议所使用的代理服务器，第二行指定FTP协议所使用的代理服务器。</p>

<p>WGet使用指南
wget是一个从网络上自动下载文件的自由工具。它支持HTTP，HTTPS和FTP协议，可以使用HTTP代理.</p>

<p>所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。</p>

<p>wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。</p>

<p>wget 非常稳定,它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完  毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。</p>

<p>wget的常见用法
wget的使用格式</p>

<p>Usage: wget [OPTION]&#8230; [URL]&#8230;用wget做站点镜像:
wget -r -p -np -k http://dsec.pku.edu.cn/~us&#8230;</p>

<h1>或者</h1>

<p>wget -m http://www.tldp.org/LDP/ab&#8230;在不稳定的网络上下载一个部分下载的文件，以及在空闲时段下载
wget -t 0 -w 31 -c http://dsec.pku.edu.cn/BBC&#8230; -o down.log &amp;</p>

<h1>或者从filelist读入要下载的文件列表</h1>

<p>wget  -t 0 -w 31 -c -B ftp://dsec.pku.edu.cn/linu&#8230; -i filelist.txt -o  down.log  &amp;上面的代码还可以用来在网络比较空闲的时段进行下载。我的用法是:在mozilla中将不方便当时下载的URL链接拷贝到内存中然后粘贴到文件 filelist.txt中，在晚上要出去系统前执行上面代码的第二条。</p>

<p>使用代理下载
wget -Y on -p -k https://sourceforge.net/pr&#8230;代理可以在环境变量或wgetrc文件中设定</p>

<h1>在环境变量中设定代理</h1>

<p>export PROXY=http://211.90.168.94:8080/</p>

<h1>在~/.wgetrc中设定代理</h1>

<p>http_proxy = http://proxy.yoyodyne.com:&#8230;
ftp_proxy = http://proxy.yoyodyne.com:&#8230;各种选项分类列表
启动
-V,  &#8211;version           显示wget的版本后退出
-h,  &#8211;help              打印语法帮助
-b,  &#8211;background        启动后转入后台执行
-e,  &#8211;execute=COMMAND   执行<code>.wgetrc'格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc记录和输入文件
-o,  --output-file=FILE     把记录写到FILE文件中
-a,  --append-output=FILE   把记录追加到FILE文件中
-d,  --debug                打印调试输出
-q,  --quiet                安静模式(没有输出)
-v,  --verbose              冗长模式(这是缺省设置)
-nv, --non-verbose          关掉冗长模式，但不是安静模式
-i,  --input-file=FILE      下载在FILE文件中出现的URLs
-F,  --force-html           把输入文件当作HTML格式文件对待
-B,  --base=URL             将URL作为在-F -i参数指定的文件中出现的相对链接的前缀
--sslcertfile=FILE     可选客户端证书
--sslcertkey=KEYFILE   可选客户端证书的KEYFILE
--egd-file=FILE        指定EGD socket的文件名下载
--bind-address=ADDRESS   指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)
-t,  --tries=NUMBER           设定最大尝试链接次数(0 表示无限制).
-O   --output-document=FILE   把文档写到FILE文件中
-nc, --no-clobber             不要覆盖存在的文件或使用.#前缀
-c,  --continue               接着下载没下载完的文件
--progress=TYPE          设定进程条标记
-N,  --timestamping           不要重新下载文件除非比本地文件新
-S,  --server-response        打印服务器的回应
--spider                 不下载任何东西
-T,  --timeout=SECONDS        设定响应超时的秒数
-w,  --wait=SECONDS           两次尝试之间间隔SECONDS秒
--waitretry=SECONDS      在重新链接之间等待1...SECONDS秒
--random-wait            在下载之间等待0...2*WAIT秒
-Y,  --proxy=on/off           打开或关闭代理
-Q,  --quota=NUMBER           设置下载的容量限制
--limit-rate=RATE        限定下载输率目录
-nd  --no-directories            不创建目录
-x,  --force-directories         强制创建目录
-nH, --no-host-directories       不创建主机目录
-P,  --directory-prefix=PREFIX   将文件保存到目录 PREFIX/...
--cut-dirs=NUMBER           忽略 NUMBER层远程目录HTTP 选项
--http-user=USER      设定HTTP用户名为 USER.
--http-passwd=PASS    设定http密码为 PASS.
-C,  --cache=on/off        允许/不允许服务器端的数据缓存 (一般情况下允许).
-E,  --html-extension      将所有text/html文档以.html扩展名保存
--ignore-length       忽略</code>Content-Length&#8217;头域
&#8211;header=STRING       在headers中插入字符串 STRING
&#8211;proxy-user=USER     设定代理的用户名为 USER
&#8211;proxy-passwd=PASS   设定代理的密码为 PASS
&#8211;referer=URL         在HTTP请求中包含 <code>Referer: URL'头
-s,  --save-headers        保存HTTP头到文件
-U,  --user-agent=AGENT    设定代理的名称为 AGENT而不是 Wget/VERSION.
--no-http-keep-alive  关闭 HTTP活动链接 (永远链接).
--cookies=off         不使用 cookies.
--load-cookies=FILE   在开始会话前从文件 FILE中加载cookie
--save-cookies=FILE   在会话结束后将 cookies保存到 FILE文件中FTP 选项
-nr, --dont-remove-listing   不移走</code>.listing&#8217;文件
-g,  &#8211;glob=on/off           打开或关闭文件名的 globbing机制
&#8211;passive-ftp           使用被动传输模式 (缺省值).
&#8211;active-ftp            使用主动传输模式
&#8211;retr-symlinks         在递归的时候，将链接指向文件(而不是目录)递归下载
-r,  &#8211;recursive          递归下载－－慎用!
-l,  &#8211;level=NUMBER       最大递归深度 (inf 或 0 代表无穷).
&#8211;delete-after       在现在完毕后局部删除文件
-k,  &#8211;convert-links      转换非相对链接为相对链接
-K,  &#8211;backup-converted   在转换文件X之前，将之备份为 X.orig
-m,  &#8211;mirror             等价于 -r -N -l inf -nr.
-p,  &#8211;page-requisites    下载显示HTML文件的所有图片递归下载中的包含和不包含(accept/reject)
-A,  &#8211;accept=LIST                分号分隔的被接受扩展名的列表
-R,  &#8211;reject=LIST                分号分隔的不被接受的扩展名的列表
-D,  &#8211;domains=LIST               分号分隔的被接受域的列表
&#8211;exclude-domains=LIST       分号分隔的不被接受的域的列表
&#8211;follow-ftp                 跟踪HTML文档中的FTP链接
&#8211;follow-tags=LIST           分号分隔的被跟踪的HTML标签的列表
-G,  &#8211;ignore-tags=LIST           分号分隔的被忽略的HTML标签的列表
-H,  &#8211;span-hosts                 当递归时转到外部主机
-L,  &#8211;relative                   仅仅跟踪相对链接
-I,  &#8211;include-directories=LIST   允许目录的列表
-X,  &#8211;exclude-directories=LIST   不被包含目录的列表
-np, &#8211;no-parent                  不要追溯到父目录</p>

<p>Wget使用技巧</p>

<p>来源：Linux技术中坚站</p>

<p>wget的使用形式是：
wget [参数列表] URL
首先来介绍一下wget的主要参数：
· -b：让wget在后台运行，记录文件写在当前目录下&#8221;wget-log&#8221;文件中；
· -t [nuber of times]：尝试次数，当wget无法与服务器建立连接时，尝试连接多少次
。比如&#8221;-t
120&#8221;表示尝试120次。当这一项为&#8221;0&#8221;的时候，指定尝试无穷多次直到连接成功为止，这个
设置非常有用，当对方服务器突然关机或者网络突然中断的时候，可以在恢复正常后继续
下载没有传完的文件；
· -c：断点续传，这也是个非常有用的设置，特别当下载比较大的文件的时候，如果中
途意外中断，那么连接恢复的时候会从上次没传完的地方接着传，而不是又从头开始，使
用这一项需要远程服务器也支持断点续传，一般来讲，基于UNIX/Linux的Web/FTP服务器
都支持断点续传；
· -T [number of seconds]：超时时间，指定多长时间远程服务器没有响应就中断连接
，开始下一次尝试。比如&#8221;-T
120&#8221;表示如果120秒以后远程服务器没有发过来数据，就重新尝试连接。如果网络速度比
较快，这个时间可以设置的短些，相反，可以设置的长一些，一般最多不超过900，通常
也不少于60，一般设置在120左右比较合适；
· -w [number of seconds]：在两次尝试之间等待多少秒，比如&#8221;-w 100&#8221;表示两次尝试
之间等待100秒；
· -Y on/off：通过／不通过代理服务器进行连接；
· -Q [byetes]：限制下载文件的总大小最多不能超过多少，比如&#8221;-Q2k&#8221;表示不能超过2K
字节，&#8221;-Q3m&#8221;表示最多不能超过3M字节，如果数字后面什么都不加，就表示是以字节为单
位，比如&#8221;-Q200&#8221;表示最多不能超过200字节；
· -nd：不下载目录结构，把从服务器所有指定目录下载的文件都堆到当前目录里；
· -x：与&#8221;-nd&#8221;设置刚好相反，创建完整的目录结构，例如&#8221;wget -nd
http://www.gnu.org&#8221;将创建在当前目录下创建&#8221;www.gnu.org&#8221;子目录，然后按照服务器
实际的目录结构一级一级建下去，直到所有的文件都传完为止；
· -nH：不创建以目标主机域名为目录名的目录，将目标主机的目录结构直接下到当前目
录下；
· &#8211;http-user=username
· &#8211;http-passwd=password：如果Web服务器需要指定用户名和口令，用这两项来设定；
· &#8211;proxy-user=username
· &#8211;proxy-passwd=password：如果代理服务器需要输入用户名和口令，使用这两个选项
；
· -r：在本机建立服务器端目录结构；
· -l [depth]：下载远程服务器目录结构的深度，例如&#8221;-l 5&#8221;下载目录深度小于或者等
于5以内的目录结构或者文件；
· -m：做站点镜像时的选项，如果你想做一个站点的镜像，使用这个选项，它将自动设
定其他合适的选项以便于站点镜像；
· -np：只下载目标站点指定目录及其子目录的内容。这也是一个非常有用的选项，我们
假设某个人的个人主页里面有一个指向这个站点其他人个人主页的连接，而我们只想下载
这个人的个人主页，如果不设置这个选项，甚至&#8211;有可能把整个站点给抓下来，这显然是
我们通常不希望的；
ü 如何设定wget所使用的代理服务器
wget可以使用用户设置文件&#8221;.wgetrc&#8221;来读取很多设置，我们这里主要利用这个文件来是
设置代理服务器。使用者用什么用户登录，那么什么用户主目录下的&#8221;.wgetrc&#8221;文件就起
作用。例如，&#8221;root&#8221;用户如果想使用&#8221;.wgetrc&#8221;来设置代理服务器，&#8221;/root/.wgert&#8221;就起
作用，下面给出一个&#8221;.wge
trc&#8221;文件的内容，读者可以参照这个例子来编写自己的&#8221;wgetrc&#8221;文件：
http-proxy = 111.111.111.111:8080
ftp-proxy = 111.111.111.111:8080
这两行的含义是，代理服务器IP地址为：111.111.111.111，端口号为：80。第一行指定
HTTP协议所使用的代理服务器，第二行指定FTP协议所使用的代理服务器。</p>

<p>wget 使用实例：
wget是一个命令行工具，用于批量下载文件，支持HTTP和FTP。究竟比其他的工具好在哪里？看看内容吧 :)</p>

<p>wget基本上所有的Linux版本都自己带了，但是有多少人在用呢？呵呵，如果你没有用过，不妨试试。Windows下面的用户可以使用GNUwin32的项目，wget，基本功能完全一致。好吧，我们来以几个简单的例子看看wget的威力。</p>

<p>如果我们想下载ftp里面某个目录里面的所有文件，我们也可以不用ftp这个笨蛋，呵呵，可以享受cute ftp等图形化工具的拖一个目录的轻松了。如</p>

<p>wget -r ftp://10.8.8.8/movie/</p>

<p>呵呵，等吧！下完了，发觉有些不对劲，怎么出来个10.8.8.8的目录，进去看看，又是一个movie，哦，wget将目录结构和网站标题都给记录下来了，不要？？没有问题！比如说还是这个例子</p>

<p>wget -r -nd ftp://10.8.8.8/movie/</p>

<p>结果什么目录都没有了，faint！怎么会这样？呵呵，你如果想要这样就让它这样吧，否则使用</p>

<p>wget -r -nH ftp://10.8.8.8/movie/</p>

<p>恩？movie也不要？OK，那就这样</p>

<p>wget -r -nH &#8211;cut-dirs=1 ftp://10.8.8.8/movie/</p>

<p>这有什么用啊？cuteftp比他好用多了，而且，你这断了线能连吗？呵呵，不好意思，可以连</p>

<p>wget -c -r -nH &#8211;cut-dirs=1 ftp://10.8.8.8/movie/</p>

<p>但 是cuteftp能做下面的事情吗？比如，现在很多网站使用Apache建站，并不提供ftp服务，但是Apache有一个indexing功能，可以提 供一个类似于ftp的界面，好多文件我想下啊，怎么办？由于是HTTP协议，CuteFTP无能为力了，倒是flash get等有什么get  all这种功能，不知道他们对于目录处理怎么样。但是wget一点问题都没有，不信？我们拿CTAN为例（例子并不恰当，CTAN有FTP服务），我们下 载这里面所有的宏包，呵呵</p>

<p>wget -r -k http://www.txia.com/blog</p>

<p>-k表示将连接转换为本地连接。但是现在同样有上面的问题啊，那就把你需要的加上吧，另外也许你根本不需要向下走那么多层，比如，我们就要到第二层，那么</p>

<p>wget -r -l2 -k http://www.txia.com/blog</p>

<p>现在新的问题是，由于网页有一个排序功能，很讨厌，因为下载的时候把网页重复了好多次，那么我们可使用-A和-R开关控制下载类型，并且可以使用通配符，呵呵，现在随心所欲了吧</p>

<p>wget -r -R &#8217;<em>.htm</em>\?*&#8217; -k http://www.txia.com/blog</p>

<p>这次没有那种网页了吧？-R的意义在于拒绝下载匹配类型的文件，-A表示仅仅接受的文件类型，如-A &#8216;*.gif&#8217;将仅下载gif图片，如果有多个允许或者不允许，可以使用,分开。</p>

<p>那 么，我们现在在使用代理服务器，怎么办呢？呵呵，很高兴你选择了wget，你可以使用它的配置文件，环境变量来利用代理。这里推荐使用环境变量，如在  bash里面我们可以把天天用的proxy加到.bash_profile里面，这是Linux标准写法（很多软件都用的，什么apt-get，yum等 等）</p>

<p>export http_proxy=http://10.20.30.40:8080</p>

<p>然后，proxy就默认打开了，如果需要暂时关闭，可以使用</p>

<p>wget &#8211;proxy=off -r -k http://www.txia.com/blog</p>

<p>当然，写一个.wgetrc文件也可以，该文件可以从/usr/local/etc里面找到，里面有很详细的注释，我就不多说了。</p>

<p>下载网页的时候比较麻烦的事情是，有的网页被同时指向了很多遍，那么为了避免多次下载，我们使用</p>

<p>wget -nc -r -k http://www.txia.com/blog</p>

<p>可以避免这件事情。为了不被有的连接指向非http://www.txia.com/blog内层目录，我们还应该加上</p>

<p>wget -nc -np -r -k http://www.txia.com/blog</p>

<p>避免下载非该目录里面的文件，这也避免了到不同的host上面去。当然，如果你希望有这个功能，在多个host之间跳来跳去的下载，可以使用</p>

<p>wget -nc -np -H -r -k http://www.txia.com/blog</p>

<p>使得可以在多个host之间span，同时-I和-X可以使得我们仅仅跟踪某些目录或者不跟踪某些目录。如果某些HTML里面你需要的东西不是由这种东西作出来的，你就得使用&#8211;follow-tags和&#8211;ignore-tags了。</p>

<p>嘿，我有个文件里面都是连接，怎么办？要是不是html你只需要</p>

<p>wget -i your.file</p>

<p>如果是，那也不繁</p>

<p>wget -F -i your.file</p>

<p>wget 使用指南
wget是一个从网络上自动下载文件的自由工具。它支持HTTP，HTTPS和FTP协议，可以使用HTTP代理.</p>

<p>所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。</p>

<p>wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作&#8221;递归下载&#8221;。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。</p>

<p>wget 非常稳定,它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务 器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。</p>

<p>wget的常见用法
wget的使用格式</p>

<p>Usage: wget [OPTION]&#8230; [URL]&#8230;</p>

<p>用wget做站点镜像:
wget -r -p -np -k http://dsec.pku.edu.cn/~us&#8230;
-r   表示递归下载,会下载所有的链接,不过要注意的是,不要单独使用这个参数,因为如果你要下载的网站也有别的网站的链接,wget也会把别的网站的东西下载 下来,所以要加上 -np这个参数,表示不下载别的站点的链接.  -k表示将下载的网页里的链接修改为本地链接.-p获得所有显示网页所需的元素,比如图片什么的.</p>

<h1>或者</h1>

<p>wget -m http://www.tldp.org/LDP/ab&#8230;</p>

<p>在不稳定的网络上下载一个部分下载的文件，以及在空闲时段下载
wget -t 0 -w 31 -c http://dsec.pku.edu.cn/BBC&#8230; -o down.log &amp;</p>

<h1>或者从filelist读入要下载的文件列表</h1>

<p>wget -t 0 -w 31 -c -B ftp://dsec.pku.edu.cn/linu&#8230; -i filelist.txt -o down.log &amp;</p>

<p>上面的代码还可以用来在网络比较空闲的时段进行下载。我的用法是:在mozilla中将不方便当时下载的URL链接拷贝到内存中然后粘贴到文件filelist.txt中，在晚上要出去系统前执行上面代码的第二条。</p>

<p>使用代理下载
wget -Y on -p -k https://sourceforge.net/pr&#8230;</p>

<p>代理可以在环境变量或wgetrc文件中设定</p>

<h1>在环境变量中设定代理</h1>

<p>export PROXY=http://211.90.168.94:8080/</p>

<h1>在~/.wgetrc中设定代理</h1>

<p>http_proxy = http://proxy.yoyodyne.com:&#8230;
ftp_proxy = http://proxy.yoyodyne.com:&#8230;</p>

<p>wget各种选项分类列表
启动
-V,  &#8211;version           显示wget的版本后退出
-h,  &#8211;help              打印语法帮助
-b,  &#8211;background        启动后转入后台执行
-e,  &#8211;execute=COMMAND   执行`.wgetrc&#8217;格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc</p>

<p>记录和输入文件
-o,  &#8211;output-file=FILE     把记录写到FILE文件中
-a,  &#8211;append-output=FILE   把记录追加到FILE文件中
-d,  &#8211;debug                打印调试输出
-q,  &#8211;quiet                安静模式(没有输出)
-v,  &#8211;verbose              冗长模式(这是缺省设置)
-nv, &#8211;non-verbose          关掉冗长模式，但不是安静模式
-i,  &#8211;input-file=FILE      下载在FILE文件中出现的URLs
-F,  &#8211;force-html           把输入文件当作HTML格式文件对待
-B,  &#8211;base=URL             将URL作为在-F -i参数指定的文件中出现的相对链接的前缀
&#8211;sslcertfile=FILE     可选客户端证书
&#8211;sslcertkey=KEYFILE   可选客户端证书的KEYFILE
&#8211;egd-file=FILE        指定EGD socket的文件名</p>

<p>下载
&#8211;bind-address=ADDRESS   指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)
-t,  &#8211;tries=NUMBER           设定最大尝试链接次数(0 表示无限制).
-O   &#8211;output-document=FILE   把文档写到FILE文件中
-nc, &#8211;no-clobber             不要覆盖存在的文件或使用.#前缀
-c,  &#8211;continue               接着下载没下载完的文件
&#8211;progress=TYPE          设定进程条标记
-N,  &#8211;timestamping           不要重新下载文件除非比本地文件新
-S,  &#8211;server-response        打印服务器的回应
&#8211;spider                 不下载任何东西
-T,  &#8211;timeout=SECONDS        设定响应超时的秒数
-w,  &#8211;wait=SECONDS           两次尝试之间间隔SECONDS秒
&#8211;waitretry=SECONDS      在重新链接之间等待1&#8230;SECONDS秒
&#8211;random-wait            在下载之间等待0&#8230;2*WAIT秒
-Y,  &#8211;proxy=on/off           打开或关闭代理
-Q,  &#8211;quota=NUMBER           设置下载的容量限制
&#8211;limit-rate=RATE        限定下载输率</p>

<p>目录
-nd  &#8211;no-directories            不创建目录
-x,  &#8211;force-directories         强制创建目录
-nH, &#8211;no-host-directories       不创建主机目录
-P,  &#8211;directory-prefix=PREFIX   将文件保存到目录 PREFIX/&#8230;
&#8211;cut-dirs=NUMBER           忽略 NUMBER层远程目录</p>

<p>HTTP 选项
&#8211;http-user=USER      设定HTTP用户名为 USER.
&#8211;http-passwd=PASS    设定http密码为 PASS.
-C,  &#8211;cache=on/off        允许/不允许服务器端的数据缓存 (一般情况下允许).
-E,  &#8211;html-extension      将所有text/html文档以.html扩展名保存
&#8211;ignore-length       忽略 <code>Content-Length'头域
--header=STRING       在headers中插入字符串 STRING
--proxy-user=USER     设定代理的用户名为 USER
--proxy-passwd=PASS   设定代理的密码为 PASS
--referer=URL         在HTTP请求中包含</code>Referer: URL&#8217;头
-s,  &#8211;save-headers        保存HTTP头到文件
-U,  &#8211;user-agent=AGENT    设定代理的名称为 AGENT而不是 Wget/VERSION.
&#8211;no-http-keep-alive  关闭 HTTP活动链接 (永远链接).
&#8211;cookies=off         不使用 cookies.
&#8211;load-cookies=FILE   在开始会话前从文件 FILE中加载cookie
&#8211;save-cookies=FILE   在会话结束后将 cookies保存到 FILE文件中</p>

<p>FTP 选项
-nr, &#8211;dont-remove-listing   不移走 `.listing&#8217;文件
-g,  &#8211;glob=on/off           打开或关闭文件名的 globbing机制
&#8211;passive-ftp           使用被动传输模式 (缺省值).
&#8211;active-ftp            使用主动传输模式
&#8211;retr-symlinks         在递归的时候，将链接指向文件(而不是目录)</p>

<p>递归下载
-r,  &#8211;recursive          递归下载－－慎用!
-l,  &#8211;level=NUMBER       最大递归深度 (inf 或 0 代表无穷).
&#8211;delete-after       在现在完毕后局部删除文件
-k,  &#8211;convert-links      转换非相对链接为相对链接
-K,  &#8211;backup-converted   在转换文件X之前，将之备份为 X.orig
-m,  &#8211;mirror             等价于 -r -N -l inf -nr.
-p,  &#8211;page-requisites    下载显示HTML文件的所有图片</p>

<p>递归下载中的包含和不包含(accept/reject)
-A,  &#8211;accept=LIST                分号分隔的被接受扩展名的列表
-R,  &#8211;reject=LIST                分号分隔的不被接受的扩展名的列表
-D,  &#8211;domains=LIST               分号分隔的被接受域的列表
&#8211;exclude-domains=LIST       分号分隔的不被接受的域的列表
&#8211;follow-ftp                 跟踪HTML文档中的FTP链接
&#8211;follow-tags=LIST           分号分隔的被跟踪的HTML标签的列表
-G,  &#8211;ignore-tags=LIST           分号分隔的被忽略的HTML标签的列表
-H,  &#8211;span-hosts                 当递归时转到外部主机
-L,  &#8211;relative                   仅仅跟踪相对链接
-I,  &#8211;include-directories=LIST   允许目录的列表
-X,  &#8211;exclude-directories=LIST   不被包含目录的列表
-np, &#8211;no-parent                  不要追溯到父目录</p>

<p>问题
在递归下载的时候，遇到目录中有中文的时候，wget创建的本地目录名会用URL编码规则处理。如&#8221;天网防火墙&#8221;会被存为&#8221;%CC%EC%CD%F8%B7%C0%BB%F0%C7%BD&#8221;,这造成阅读上的极大不方便</p>

<p>前几天用wget下了一些东西，发现一些很用的方法，记录一下。</p>

<p>用wget下载整个目录：</p>

<div id="highlighter_183963">
<div>
<div>
<table><tbody><tr>
<td></td>
<td>
<code>wget -t0 </code><code>-c</code> <code>-nH</code> <code>-np</code> <code>-b</code> <code>-m</code> <code>-P</code> <code>/localdir <a href="http://destinationdirectory/">http://destinationdirectory</a> </code><code>-o</code> <code>wget.log</code>
</td>
</tr></tbody></table>
</div>
</div>
</div>


<p>参数的说明如下：</p>

<ul>
<li>
<strong>-t number (&<a href="http://www.finalbug.org/tag/8211">8211</a>;tries=number)</strong> 重试次数，默认为20，设置成0或inf表示不限制重试次数。如果收到“connection refused”或“not found”（404）错误则不再重试。</li>
    <li>
<strong>-c （&<a href="http://www.finalbug.org/tag/8211">8211</a>;continue）</strong>续传</li>
    <li>
<strong>-nH （&<a href="http://www.finalbug.org/tag/8211">8211</a>;no-host-directories）</strong>禁止host前缀。默认情况下，执行“Wget -r http://xyz/”创建的目录结构将以“xyz”开始，该选项将禁止该功能。</li>
    <li>
<strong>-np（&<a href="http://www.finalbug.org/tag/8211">8211</a>;no-parent）</strong>在递归的时候忽略父级目录。</li>
    <li>
<strong>-b（&<a href="http://www.finalbug.org/tag/8211">8211</a>;background）</strong>启动之后在后台运行。通过-o可以设置记录文件，如果没有设置，默认将记录在wget-log文件里。</li>
    <li>
<strong>-m（&<a href="http://www.finalbug.org/tag/8211">8211</a>;mirror）</strong>寻找镜像。</li>
    <li>
<strong>-P prefix（&<a href="http://www.finalbug.org/tag/8211">8211</a>;directory-prefix=prefix）</strong>将“prefix”设置为下载目录的前缀。即将所有内容下载到prefix路径下。（注意，P大写）</li>
</ul>


<p>用wget递归下载整站：</p>

<div id="highlighter_145938">
<div>
<div>
<a title="view source" href="http://www.finalbug.org/2010/12/1753/#viewSource">view source</a><a title="print" href="http://www.finalbug.org/2010/12/1753/#printSource">print</a><a title="?" href="http://www.finalbug.org/2010/12/1753/#about">?</a>
</div>
</div>
<div>
<div>
<table><tbody><tr>
<td><code>1</code></td>
<td>
<code>wget </code><code>-r</code> <code>-p</code> <code>-np</code> <code>-k</code> <code><a href="http://xxx.com/abc/">http://xxx.com/abc/</a></code>
</td>
</tr></tbody></table>
</div>
</div>
</div>


<p>参数说明如下：</p>

<ul>
<li> <strong>-p（&<a href="http://www.finalbug.org/tag/8211">8211</a>;page-requisites）</strong>下载指定的HTML文件的所有相关内容，包括图片，声音，CSS文件。（注意，p小写）</li>
    <li>
<strong>-r（&<a href="http://www.finalbug.org/tag/8211">8211</a>;recursive）</strong>递归下载。</li>
    <li>
<strong>-k（&<a href="http://www.finalbug.org/tag/8211">8211</a>;convert-links）</strong>下载完成后，将下载文件中的链接转换成本地连接，包括图片路径，CSS等等。但是指向下载目录之外的链接将不会被修改。</li>
</ul>

</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-09T00:00:00+08:00" pubdate data-updated="true">Nov 9<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/linux/'>Linux</a>


</div>
	
	<div class="comments"><a href="#disqus_thread">Comments</a></div>
	
</div>
</article>

	<div class="share">
	<div class="addthis_toolbox addthis_default_style ">
		
		
		
		
	</div>
	
</div>



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>
</div>
	<footer id="footer" class="inner">Copyright &copy; 2013

    Ocean

<div style="display:none"> <script language="javascript" type="text/javascript" src="http://js.users.51.la/4388679.js"></script>
<noscript><a href="http://www.51.la/?4388679" target="_blank"><img alt="&#x6211;&#x8981;&#x5566;&#x514D;&#x8D39;&#x7EDF;&#x8BA1;" src="http://img.users.51.la/4388679.asp" style="border:none" /></a></noscript> </div></footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->


<script type="text/javascript">
      var disqus_shortname = 'oceanblog';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://blog.nwy.me/blog/2010/11/09/wget/';
        var disqus_url = 'http://blog.nwy.me/blog/2010/11/09/wget/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//go.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





</body>
</html>

<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>Ocean's Blog</title>
	<meta name="author" content="Ocean">

	
	<meta name="description" content="0cean的个人博客，一些随笔，记录生活点滴。">
	<meta name="keywords" content="学习,技术,工作,网络,随笔,个人博客">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Ocean's Blog" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script async="true" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	
</head>


<body>
	<header id="header" class="inner"><h1><a href="/">Ocean's Blog</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/">主页</a></li>
	<li><a href="/blog/archives">存档</a></li>
	<li><a href="/blog/categories">分类</a></li>
	<li><a href="/blog/about">关于</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/">主页</a></li>
	<li><a href="/blog/archives">存档</a></li>
	<li><a href="/blog/categories">分类</a></li>
	<li><a href="/blog/about">关于</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="http://google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:blog.nwy.me">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
    
	</div>
	<form class="search" action="http://google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:blog.nwy.me">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner">


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/19/ubuntu-installation-partition-proposal/">
		
			Ubuntu安装分区建议方案</a>
	</h2>
	<div class="entry-content">
		<p>单linux系统的分区方案
这种情况下无需建立扩展分区，一块硬盘可以全部分成四个主分区，MBR和所有分区信息全部保存在第一个512B的扇区里；查找分区信息最快。
本人笔记本硬盘160G，具体分区方案如下：
sda1&#8212;20G,ext4，挂载至／目录；如果所用硬盘较小，／分区可以小到7G，不建议这样做，如果硬盘够大，就没必要省这几个G了，大点以后装大软件有余地；
sda2&#8212;256MB-512MB-1024MB-2048MB(请根据内存大小选择，为物理内存大小的1-2倍，但一般不要超过2048MB)，swap格式，无需挂载点；
sda3&#8212;1G-5G-8G（如果硬盘不够大，又不需刻录DVD，选1G，否则用5G为佳）,ext4，挂载至／tmp目录，硬盘大的可多分几个G，备份时方便点,本人分了了18G，考虑经常备份系统你，多分点一以后不够用分就比较复杂；
sda4&#8212;120G所有剩余磁盘空间,ext4，挂载至／home目录；
请注意分区的顺序、设备符和各目录挂载的次序，依次是/，swap，/tmp，/home；平均磁头移臂次数较少，读写综合性能较优。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-19T00:00:00+08:00" pubdate data-updated="true">Nov 19<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/linux/'>Linux</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/19/ubuntu-installation-partition-proposal/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/16/conky-configuration-under-ubuntu/">
		
			Ubuntu下的conky配置</a>
	</h2>
	<div class="entry-content">
		<p>Conky是一种自由软件，用于X视窗系统的系统监视，可以在FreeBSD、OpenBSD和各种Linux发布上使用的自由软件。Conky具有很高的可配置性，可以监视许多系统参数，如：CPU、内存、交换内存、硬盘使用情况等状态；各种硬件的温度；系统的进程（top）；网络状态；电池电量；系统信息和邮件收发；各种音乐播放器MPD、XMMS2、BMPx、Audacious）的控制。不像其他系统监视器那样需要高级别的部件工具箱（widget toolkits）来渲染他们的信息，Conky可以直接在X视窗下渲染，这意味着在相同配置下Conky可以消耗更少的资源。</p>

<p><span style="color: #ff0000;">本文的最后提供了截图中的conky配置文件以及桌面的下载.</span></p>

<p>安装conky</p>

<p>ocean@ocean-ThinkPad-T61:~$ sudo apt-get install conky</p>

<p>conky默认没有提供配置文件，需要使用者手动创建配置：</p>

<p>ocean@ocean-ThinkPad-T61:~$ gedit<code> ~/.conkyrc</code></p>

<p>复制下面的代码,并保存,在终端运行conky</p>

<blockquote>
<pre># .conkyrc - Edited from Russian Conky from Gnome-Look.org
# Edited by Alpha-Thinker
# Made specifically for the Lenovo T61 with NVIDIA Quadro NVS 140M discrete graphics

# --- Window Layout & Options --- #
own_window yes
own_window_colour brown
own_window_transparent yes
own_window_type normal
own_window_transparent yes
own_window_hints undecorated,below,sticky,skip_taskbar,skip_pager
double_buffer yes
use_spacer right
use_xft yes
alignment top_right
gap_x 20
gap_y 45

# --- Colours, Sizes, Fonts & Margins --- #
update_interval 1.0
maximum_width 250
stippled_borders 3
border_margin 9
border_width 10
default_color white

# --- Text --- #
draw_outline no
draw_borders no
font Sans:size=8:weight=bold
uppercase no
draw_shades yes
override_utf8_locale yes

TEXT
${font Sans:size=14:weight=bold}${color red} ${alignc} ${time %r}
${font Sans:size=11:weight=bold}${color white} ${alignc} ${time %A} ${time %e} ${time %B} ${time %G}

${font Sans:size=9:weight=bold}${color orange}SYSTEM INFO${hr 2}$color${font Sans:size=8:weight=bold}
${color orange}Machine$color Thinkpad T61 ${alignr}${color orange} Uptime$color $uptime
${color orange}Kernel$color  $kernel ${alignr}${color orange}Type$color $machine

${font Sans:size=9:weight=bold}${color orange}Processor ${hr 2}$color
${font Arial:bold:size=8}${color #ff0000}${execi 99999 cat /proc/cpuinfo | grep "model name" -m1 | cut -d":" -f2 | cut -d" " -f2- | sed 's#Processor ##'}$font$color
${color orange}Speed:$color${execi 20 sensors |grep "Core0 Temp" | cut -d" " -f4}$font$color$alignr${freq_g 2}Ghz         ${color #c0ff3e}${execi 20 sensors |grep "Core1 Temp" | cut -d" " -f4}  $color${alignr}${color orange}Processes:$color $running_processes/ $processes

${font Sans:size=9:weight=bold}${color orange}CPU Usage ${hr 2}$color
${color white}Core 1   ${color red}${cpu cpu0}%           ${color white}Core 2   ${color red}${cpu cpu1}% $color
${cpugraph cpu0 25,120 000000 ff6600 }  ${cpugraph cpu1 25,120 000000 ff6600 }
${font Sans:size=8:weight=bold}${color red}CPU Temp${color white}              ${acpitemp}小$color
${font Sans:size=8:weight=bold}${color red}NVIDIA GPU Temp ${color white}  ${execi 30 nvidia-settings -q gpucoretemp | grep '):' | awk '{print $4}' | cut -c -2} C$color
${font Sans:size=8:weight=bold}${color red}FAN SPEED${color white}             ${ibm_fan} rpm$color

${font Sans:size=9:weight=bold}${color orange}TOP 5 CPU Users ${hr 2}$color${font Sans:size=8:weight=bold}${color #ff0000}
Process                          ${alignr}ID      ${alignr}CPU  $color
1. ${top name 1}     ${alignr}${top pid 1}   ${alignr}${top cpu 1}
2. ${top name 2}     ${alignr}${top pid 2}   ${alignr}${top cpu 2}
3. ${top name 3}     ${alignr}${top pid 3}   ${alignr}${top cpu 3}
4. ${top name 4}     ${alignr}${top pid 4}   ${alignr}${top cpu 4}
5. ${top name 5}     ${alignr}${top pid 5}   ${alignr}${top cpu 5}

${font Sans:size=9:weight=bold}${color orange}TOP 5 RAM Users ${hr 2}$color${font Sans:size=8:weight=bold}${color #ff0000}
Process                             ${alignr}ID      ${alignr}RAM $color
1. ${top_mem name 1}     ${alignr}${top_mem pid 1}   ${alignr}${top_mem mem 1}
2. ${top_mem name 2}     ${alignr}${top_mem pid 2}   ${alignr}${top_mem mem 2}
3. ${top_mem name 3}     ${alignr}${top_mem pid 3}   ${alignr}${top_mem mem 3}
4. ${top_mem name 4}     ${alignr}${top_mem pid 4}   ${alignr}${top_mem mem 4}
5. ${top_mem name 5}     ${alignr}${top_mem pid 5}   ${alignr}${top_mem mem 5}

${font Sans:size=9:weight=bold}${color orange}RAM & SWAP ${hr 2}$color${font Sans:size=8:weight=bold}
${color white}RAM$color  ${memperc}%  ${color #ff6600}${membar 3.180}
${color white}SWAP$color  ${swapperc}%  ${color #ff6600}${swapbar 3.180}

${font Sans:size=9:weight=bold}${color orange}LAN (IP: ${addr eth0}) ${hr 2}$color${font Sans:size=8:weight=bold}
${color white}Down$color ${downspeed eth0}/s${alignr}${color white}Up$color${alignr}    ${upspeed eth0}/s
${downspeedgraph eth0 25,120 000000 00ff00} ${alignr}${upspeedgraph eth0 25,120 000000 ff0000}$color
${font Sans:size=9:weight=bold}${color orange}Wi-fi (IP: ${addr wlan0}) ${hr 2}$color${font Sans:size=8:weight=bold}
${color white}Down$color ${downspeed wlan0}/s${alignr}${color white}Up$color${alignr}    ${upspeed wlan0}/s
${downspeedgraph wlan0 25,120 000000 00ff00} ${alignr}${upspeedgraph wlan0 25,120 000000 ff0000}$color
${font Sans:size=9:weight=bold}${color orange}GSM/3G (IP: ${addr ppp0}) ${hr 2}$color${font Sans:size=8:weight=bold}
${color white}Down$color ${downspeed ppp0}/s${alignr}${color white}Up$color${alignr}    ${upspeed ppp0}/s
${downspeedgraph ppp0 25,120 000000 00ff00} ${alignr}${upspeedgraph ppp0 25,120 000000 ff0000}$color
</pre>
</blockquote>


<p>本文最后有文章中的截图的conky配置文件的下载，如果想更进一步定制需要使用者自己动手，当然网上论坛也有大量的配置可以参考。</p>

<p>配置的语法定义在软件官方网站有说明，在本地&#8221;/usr/share/doc/conky-all/&#8221;路径下也有帮助文档。</p>

<p>一些网友提供的配置截图以及相应的配置文件，http://conky.sourceforge.net/screenshots.html</p>

<p>附一张自己定制的conky截图</p>

<p><a href="http://nwy.me/img/2010/11/0cean.jpg"><img class="alignnone size-medium wp-image-62054" title="ocean" src="http://nwy.me/img/2010/11/0cean-97x300.jpg" alt="" width="97" height="300"></a><a href="http://nwy.me/img/2010/11/conky.png"><img class="alignnone size-medium wp-image-62058" title="conky" src="http://nwy.me/img/2010/11/conky-300x225.png" alt="" width="300" height="225"></a></p>

<p>右边的截图是网友评论最帅的conky配置，配置文件见 <a rel="nofollow" href="http://blog.brixandersen.dk/?p=67">http://blog.brixandersen.dk/?p=67</a></p>

<p>配置文件下载:<a href="..http://nwy.me/img/2010/11/conkyrc.tar.gz">.conkyrc.tar</a></p>

<p>壁纸下载:<a href="http://nwy.me/img/2010/11/conky.tar.gz">conky.tar</a></p>

<p>conky默认不会开机自动运行可以创建一个文件,复制下面的内容,并保存为myconky.sh,在系统/首选项/启动应用程序里添加这个文件,延迟十五秒是为了避免conky先于Gonme运行,而无法显示.</p>

<blockquote>#!/bin/bash
sleep 15 && conky</blockquote>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-16T00:00:00+08:00" pubdate data-updated="true">Nov 16<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/linux/'>Linux</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/16/conky-configuration-under-ubuntu/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/13/note/">
		
			风雨20年：我所积累的20条编程经验</a>
	</h2>
	<div class="entry-content">
		<p>原文作者乔纳森·丹尼可（Jonathan Danylko）是一位自由职业的web架构师和程序员，编程经验已超过20年，涉足领域有电子商务、生物技术、房地产、医疗、保险和公用事业。正如乔纳 森在文中所言，本文适合刚毕业的大学生和刚入门的程序员。如果你已是高级开发人员，或许你在本文中看到自己的身影。</p>

<div>
<a href="http://img.cnbeta.com/newsimg/101113/1102260779095972.jpg"></a><a href="http://nwy.me/img/2010/11/1102260779095972.jpg"><img class="alignnone size-medium wp-image-62044" title="1102260779095972" src="http://nwy.me/img/2010/11/1102260779095972-300x110.jpg" alt="" width="300" height="110"></a>
</div>


<p>从11岁时，我就一直在编程，并且一直都很喜欢技术和编程。这些年来，我积累了一些艰难又容易的经验。作为一名程序员，你或许还没这些经验，但我会把它们献给那些想从中学到更多的朋友。</p>

<p>我会持续更新这些经验，我可能还会有更多的感想，但就我这20年来看，我想下面这个列表中基本不需要增添额外的东西了。下面就是我至今最难忘的经验。</p>

<p><strong>1. 估算解决问题所需要的时间。</strong>不要怕，承认吧！我曾见过一些程序员为了解决一个特殊问题而坐在显示器前面8小时。为自己定一个时间限制吧，1小时、30分钟或甚至15分钟。如果在这期间你不能解决问题，那就去寻求帮助，或到网上找答案，而不是尝试去做“超级堆码员”。</p>

<p><strong>2. 编程语言是一种语言，只是一种语言。</strong>随着时光推移，只要你理解了一种语言的原理，你会发现各种语言之间的相似之处 。你所选择的语言，你应该觉得“舒服”，并且能够写出有效（而且简洁）的代码。最重要的，让语言去适应项目，反之亦然。</p>

<p><strong>3. 不要过于注重程序的“设计模式”。</strong> 有时候，写一个简单的算法，要比引入某种模式更容易。在多数情况下，程序代码应是简单易懂，甚至清洁工也能看懂。</p>

<p><strong>4. 经常备份代码。</strong>在我年轻时，我就有过因硬盘故障而丢了大量代码的经历，这经历很恐怖的。只要你一次没有备份，就应当像有着严格的期限，客户明天就需要。此时就该源码/版本控制软件大显身手了。</p>


		
		<a href="/blog/2010/11/13/note/" class="more-link">Read on &rarr;</a>
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-13T00:00:00+08:00" pubdate data-updated="true">Nov 13<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/other/'>Other</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/13/note/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/12/linux/">
		
			Linux常用信息查看命令</a>
	</h2>
	<div class="entry-content">
		<div>

<strong>系统</strong>
<pre># uname -a               # 查看内核/操作系统/CPU信息
# head -n 1 /etc/issue   # 查看操作系统版本
# cat /proc/cpuinfo      # 查看CPU信息
# hostname               # 查看计算机名
# lspci -tv              # 列出所有PCI设备
# lsusb -tv              # 列出所有USB设备
# lsmod                  # 列出加载的内核模块
# env                    # 查看环境变量</pre>
<strong>资源</strong>
<pre># free -m                # 查看内存使用量和交换区使用量
# df -h                  # 查看各分区使用情况
# du -sh <目录名>        # 查看指定目录的大小
# grep MemTotal /proc/meminfo   # 查看内存总量
# grep MemFree /proc/meminfo    # 查看空闲内存量
# uptime                 # 查看系统运行时间、用户数、负载
# cat /proc/loadavg      # 查看系统负载</pre>
<strong>磁盘和分区</strong>

<strong>
		
		<a href="/blog/2010/11/12/linux/" class="more-link">Read on &rarr;</a>
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-12T00:00:00+08:00" pubdate data-updated="true">Nov 12<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/linux/'>Linux</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/12/linux/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/11/found-an-interesting-site-facial-search-engine/">
		
			发现一个有趣的网站：面部搜索引擎</a>
	</h2>
	<div class="entry-content">
		<p><a href="http://cn.pictriev.com/facedb/fs2.php" target="_blank">Pictriev</a>是一个通过面部辨识，在网上查找任何相似的面部的网站. 网站引擎不仅可以搜索相似的面部，还可以通过其特有的算法判断照片中人脸主人的大约年龄，以及性别，用自己的照片测试了一下，对年龄的判断比较准确，性别也识别出来了，99%male，很好奇1%的到哪去了，贴一张搜索引擎识别出来的和我相似的人的照片，本人对引擎的此搜索结果不做任何评论，保留最终解释权。</p>

<p><a href="http://nwy.me/img/2010/11/2.jpg"></a><a href="http://nwy.me/img/2010/11/21.jpg"><img class="aligncenter size-thumbnail wp-image-62449" title="2" src="http://nwy.me/img/2010/11/21-150x150.jpg" alt="" width="150" height="150"></a></p>

<p>最后做总结性的陈述：这一网站的发现填补了广大网友在人肉搜索面部智能识别方面的空白，具有里程碑意义，标志着广大网友的人肉搜索技术与国内MOP，天涯等网站网友的一流技术接轨。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-11T00:00:00+08:00" pubdate data-updated="true">Nov 11<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/personal/'>Personal</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/11/found-an-interesting-site-facial-search-engine/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/10/100-best-linux-site-recommended/">
		
			100个最佳Linux站点(推荐)</a>
	</h2>
	<div class="entry-content">
		<p>这在很早就出现在LinuxByteOther的文章里面,但一直都没有完整版本,今天在非常Linux发现,立即Other给大家!
（一） 软件下载</p>

<p>(1) Freshmeat站点</p>

<p>网址：http://www.freshmeat.net</p>

<p>评介：如果你绝对肯定，非要获取开放源代码应用程序，那毫无疑问，Freshmeat就是个不错的站点。这个最值得推崇的软件下载站点，有着数以千计的大 量开放源代码应用程序的分类链接。此外，Freshmeat站点在更新程序的基础上，每天还会添加10到30个新程序链接。</p>

<p>(2) Tucows Linux</p>

<p>网址：http://linux.tucows.com</p>

<p>评介：最初，该站点是一个基于Windows的软件园地，几年后，Tucows拓宽领域。现在，该站点已经发展成为了最开放的Linux下载站点之一。不过，说真的，这也是因为Linux从一开始就增强了服务器的缘故。</p>

<p>(3) Woven Goods for Linux</p>

<p>网址：http://www.fokus.gmd.de/linux</p>

<p>评介：Woven Goods for Linux 是一个德语站点，它的特征就在于英文和德文版的Linux程序下载和文档。该站点还有一个很不错的综合列表，几乎列出了你所能想象出的每一个Linux分发。</p>


		
		<a href="/blog/2010/11/10/100-best-linux-site-recommended/" class="more-link">Read on &rarr;</a>
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-10T00:00:00+08:00" pubdate data-updated="true">Nov 10<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/other/'>Other</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/10/100-best-linux-site-recommended/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/09/wget/">
		
			Wget 使用技巧</a>
	</h2>
	<div class="entry-content">
		<p><a href="http://www.gnu.org/software/wget/">wget</a> 是一个命令行的下载工具。对于我们这些 Linux 用户来说，几乎每天都在使用它。下面为大家介绍几个有用的 wget 小技巧，可以让你更加高效而灵活的使用 wget。</p>

<ul>
<li><code>$ wget -r -np -nd http://example.com/packages/</code></li>
</ul>


<p>这条命令可以下载 http://example.com 网站上 packages 目录中的所有文件。其中，<code>-np</code> 的作用是不遍历父目录，<code>-nd</code> 表示不在本机重新创建目录结构。</p>

<ul>
<li><code>$ wget -r -np -nd --accept=iso http://example.com/centos-5/i386/</code></li>
</ul>


<p>与上一条命令相似，但多加了一个 <code>--accept=iso</code> 选项，这指示 wget 仅下载 i386 目录中所有扩展名为 iso 的文件。你也可以指定多个扩展名，只需用逗号分隔即可。</p>

<ul>
<li><code>$ wget -i filename.txt</code></li>
</ul>


<p>此命令常用于批量下载的情形，把所有需要下载文件的地址放到 filename.txt 中，然后 wget 就会自动为你下载所有文件了。</p>

<ul>
<li><code>$ wget -c http://example.com/really-big-file.iso</code></li>
</ul>


<p>这里所指定的 <code>-c</code> 选项的作用为断点续传。</p>

<ul>
<li><code>$ wget -m -k (-H) http://www.example.com/</code></li>
</ul>


<p>该命令可用来镜像一个网站，wget 将对链接进行转换。如果网站中的图像是放在另外的站点，那么可以使用 <code>-H</code> 选项。</p>

<p>以上文字Other自<a href="http://linuxtoy.org/archives/wget-tips.html">这里</a>。</p>

<p>补充整理：</p>

<p>wget的主要特点包括：</p>

<ul>
<li>支持递归下载</li>
    <li>恰当的转换页面中的链接</li>
    <li>生成可在本地浏览的页面镜像</li>
    <li>支持代理服务器</li>
</ul>


<p>在其之上的图形界面应用程序有：<a title="GNOME" href="http://zh.wikipedia.org/wiki/GNOME">GNOME</a>下面的<strong>gwget</strong>。Windows系统下面的<strong>wGetGUI。</strong>多线程方式,推荐用axel，多线程抓站还可以用 pavuk.国内有很多下载地址是加了跳转的 ,不能直接wget的，可以在网址上加单引号&#8221;这样就可以了。</p>

<h4>附录：<a href="http://pengjiayou.com/blog/wget-use-skills-daquan" target="_blank">wget 使用技巧大全</a>
</h4>


<p>Wget 的使用</p>

<p>1）支持断点下传功能（2）同时支持FTP和HTTP下载方式（3）支持代理服务器（4）设置方便简单；5）程序小，完全免费；</p>

<p>命令格式：</p>

<p>wget [参数列表] [目标软件、网页的网址]</p>

<p>1、启动类参数</p>

<p>这一类参数主要提供软件的一些基本信息；</p>

<p>-V,&#8211;version 显示软件版本号然后退出；
-h,&#8211;help显示软件帮助信息；
-e,&#8211;execute=COMMAND 执行一个 “.wgetrc”命令</p>

<p>以上每一个功能有长短两个参数，长短功能一样，都可以使用。需要注意的是，这里的-e参数是执行一个.wgettrc的命令，.wgettrc命令其实是一个参数列表，直接将软件需要的参数写在一起就可以了。</p>

<p>2、文件处理参数</p>

<p>这类参数定义软件log文件的输出方式等；</p>

<p>-o,&#8211;output-file=FILE 将软件输出信息保存到文件；
-a,&#8211;append-output=FILE将软件输出信息追加到文件；
-d,&#8211;debug显示输出信息；
-q,&#8211;quiet 不显示输出信息；
-i,&#8211;input-file=FILE 从文件中取得URL；</p>

<p>以上参数对于攻击者比较有用，我们来看看具体使用；</p>

<p>例1：下载192.168.1.168首页并且显示下载信息
wget -d http://192.168.1.168</p>

<p>例2：下载192.168.1.168首页并且不显示任何信息
wget -q http://192.168.1.168</p>

<p>例3：下载filelist.txt中所包含的链接的所有文件
wget -i filelist.txt</p>

<p>wget -np -m -l5  http://jpstone.bokee.com //不下载本站所链接的其它站点内容，5级目录结构
3、下载参数</p>

<p>下载参数定义下载重复次数、保存文件名等；</p>

<p>-t,&#8211;tries=NUMBER 是否下载次数（0表示无穷次）
-O &#8211;output-document=FILE下载文件保存为别的文件名
-nc, &#8211;no-clobber 不要覆盖已经存在的文件
-N,&#8211;timestamping只下载比本地新的文件
-T,&#8211;timeout=SECONDS 设置超时时间
-Y,&#8211;proxy=on/off 关闭代理</p>

<p>例：下载192.168.1.168的首页并将下载过程中的的输入信息保存到test.htm文件中
wget -o test.htm http://192.168.1.168</p>

<p>4、目录参数</p>

<p>目录参数主要设置下载文件保存目录与原来文件（服务器文件）的目录对应关系；</p>

<p>-nd  &#8211;no-directories 不建立目录
-x,&#8211;force-directories 强制建立目录
可能现在我们对这里的目录还不是很了解，我们来看一个举例</p>

<p>例：下载192.168.1.168的首页，并且保持网站结构
wget -x http://192.168.1.168</p>

<p>5、HTTP参数</p>

<p>HTTP参数设置一些与HTTP下载有关的属性；</p>

<p>&#8211;http-user=USER设置HTTP用户
&#8211;http-passwd=PASS设置HTTP密码
&#8211;proxy-user=USER设置代理用户
&#8211;proxy-passwd=PASS设置代理密码</p>

<p>以上参数主要设置HTTP和代理的用户、密码；</p>

<p>6、递归参数设置</p>

<p>在下载一个网站或者网站的一个目录的时候，我们需要知道的下载的层次，这些参数就可以设置；
-r,&#8211;recursive 下载整个网站、目录（小心使用）
-l,&#8211;level=NUMBER 下载层次</p>

<p>例：下载整个网站
wget -r http://192.168.1.168</p>

<p>7、递归允许与拒绝选项参数</p>

<p>下载一个网站的时候，为了尽量快，有些文件可以选择下载，比如图片和声音，在这里可以设置；</p>

<p>-A,&#8211;accept=LIST 可以接受的文件类型
-R,&#8211;reject=LIST拒绝接受的文件类型
-D,&#8211;domains=LIST可以接受的域名
&#8211;exclude-domains=LIST拒绝的域名
-L,&#8211;relative 下载关联链接
&#8211;follow-ftp 只下载FTP链接
-H,&#8211;span-hosts 可以下载外面的主机
-I,&#8211;include-directories=LIST允许的目录
-X,&#8211;exclude-directories=LIST 拒绝的目录</p>

<p>如何设定wget所使用的代理服务器
wget可以使用用户设置文件&#8221;.wgetrc&#8221;来读取很多设置，我们这里主要利用这个文件来是
设置代理服务器。使用者用什么用户登录，那么什么用户主目录下的&#8221;.wgetrc&#8221;文件就起
作用。例如，&#8221;root&#8221;用户如果想使用&#8221;.wgetrc&#8221;来设置代理服务器，&#8221;/root/.wgetrc&#8221;就起
作用，下面给出一个&#8221;.wgetrc&#8221;文件的内容，读者可以参照这个例子来编写自己的&#8221;wgetrc&#8221;文件：
http-proxy = 111.111.111.111:8080
ftp-proxy = 111.111.111.111:8080
这两行的含义是，代理服务器IP地址为：111.111.111.111，端口号为：80。第一行指定
HTTP协议所使用的代理服务器，第二行指定FTP协议所使用的代理服务器。</p>

<p>WGet使用指南
wget是一个从网络上自动下载文件的自由工具。它支持HTTP，HTTPS和FTP协议，可以使用HTTP代理.</p>

<p>所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。</p>

<p>wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。</p>

<p>wget 非常稳定,它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完  毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。</p>

<p>wget的常见用法
wget的使用格式</p>

<p>Usage: wget [OPTION]&#8230; [URL]&#8230;用wget做站点镜像:
wget -r -p -np -k http://dsec.pku.edu.cn/~us&#8230;</p>

<h1>或者</h1>

<p>wget -m http://www.tldp.org/LDP/ab&#8230;在不稳定的网络上下载一个部分下载的文件，以及在空闲时段下载
wget -t 0 -w 31 -c http://dsec.pku.edu.cn/BBC&#8230; -o down.log &amp;</p>

<h1>或者从filelist读入要下载的文件列表</h1>

<p>wget  -t 0 -w 31 -c -B ftp://dsec.pku.edu.cn/linu&#8230; -i filelist.txt -o  down.log  &amp;上面的代码还可以用来在网络比较空闲的时段进行下载。我的用法是:在mozilla中将不方便当时下载的URL链接拷贝到内存中然后粘贴到文件 filelist.txt中，在晚上要出去系统前执行上面代码的第二条。</p>

<p>使用代理下载
wget -Y on -p -k https://sourceforge.net/pr&#8230;代理可以在环境变量或wgetrc文件中设定</p>

<h1>在环境变量中设定代理</h1>

<p>export PROXY=http://211.90.168.94:8080/</p>

<h1>在~/.wgetrc中设定代理</h1>

<p>http_proxy = http://proxy.yoyodyne.com:&#8230;
ftp_proxy = http://proxy.yoyodyne.com:&#8230;各种选项分类列表
启动
-V,  &#8211;version           显示wget的版本后退出
-h,  &#8211;help              打印语法帮助
-b,  &#8211;background        启动后转入后台执行
-e,  &#8211;execute=COMMAND   执行<code>.wgetrc'格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc记录和输入文件
-o,  --output-file=FILE     把记录写到FILE文件中
-a,  --append-output=FILE   把记录追加到FILE文件中
-d,  --debug                打印调试输出
-q,  --quiet                安静模式(没有输出)
-v,  --verbose              冗长模式(这是缺省设置)
-nv, --non-verbose          关掉冗长模式，但不是安静模式
-i,  --input-file=FILE      下载在FILE文件中出现的URLs
-F,  --force-html           把输入文件当作HTML格式文件对待
-B,  --base=URL             将URL作为在-F -i参数指定的文件中出现的相对链接的前缀
--sslcertfile=FILE     可选客户端证书
--sslcertkey=KEYFILE   可选客户端证书的KEYFILE
--egd-file=FILE        指定EGD socket的文件名下载
--bind-address=ADDRESS   指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)
-t,  --tries=NUMBER           设定最大尝试链接次数(0 表示无限制).
-O   --output-document=FILE   把文档写到FILE文件中
-nc, --no-clobber             不要覆盖存在的文件或使用.#前缀
-c,  --continue               接着下载没下载完的文件
--progress=TYPE          设定进程条标记
-N,  --timestamping           不要重新下载文件除非比本地文件新
-S,  --server-response        打印服务器的回应
--spider                 不下载任何东西
-T,  --timeout=SECONDS        设定响应超时的秒数
-w,  --wait=SECONDS           两次尝试之间间隔SECONDS秒
--waitretry=SECONDS      在重新链接之间等待1...SECONDS秒
--random-wait            在下载之间等待0...2*WAIT秒
-Y,  --proxy=on/off           打开或关闭代理
-Q,  --quota=NUMBER           设置下载的容量限制
--limit-rate=RATE        限定下载输率目录
-nd  --no-directories            不创建目录
-x,  --force-directories         强制创建目录
-nH, --no-host-directories       不创建主机目录
-P,  --directory-prefix=PREFIX   将文件保存到目录 PREFIX/...
--cut-dirs=NUMBER           忽略 NUMBER层远程目录HTTP 选项
--http-user=USER      设定HTTP用户名为 USER.
--http-passwd=PASS    设定http密码为 PASS.
-C,  --cache=on/off        允许/不允许服务器端的数据缓存 (一般情况下允许).
-E,  --html-extension      将所有text/html文档以.html扩展名保存
--ignore-length       忽略</code>Content-Length&#8217;头域
&#8211;header=STRING       在headers中插入字符串 STRING
&#8211;proxy-user=USER     设定代理的用户名为 USER
&#8211;proxy-passwd=PASS   设定代理的密码为 PASS
&#8211;referer=URL         在HTTP请求中包含 <code>Referer: URL'头
-s,  --save-headers        保存HTTP头到文件
-U,  --user-agent=AGENT    设定代理的名称为 AGENT而不是 Wget/VERSION.
--no-http-keep-alive  关闭 HTTP活动链接 (永远链接).
--cookies=off         不使用 cookies.
--load-cookies=FILE   在开始会话前从文件 FILE中加载cookie
--save-cookies=FILE   在会话结束后将 cookies保存到 FILE文件中FTP 选项
-nr, --dont-remove-listing   不移走</code>.listing&#8217;文件
-g,  &#8211;glob=on/off           打开或关闭文件名的 globbing机制
&#8211;passive-ftp           使用被动传输模式 (缺省值).
&#8211;active-ftp            使用主动传输模式
&#8211;retr-symlinks         在递归的时候，将链接指向文件(而不是目录)递归下载
-r,  &#8211;recursive          递归下载－－慎用!
-l,  &#8211;level=NUMBER       最大递归深度 (inf 或 0 代表无穷).
&#8211;delete-after       在现在完毕后局部删除文件
-k,  &#8211;convert-links      转换非相对链接为相对链接
-K,  &#8211;backup-converted   在转换文件X之前，将之备份为 X.orig
-m,  &#8211;mirror             等价于 -r -N -l inf -nr.
-p,  &#8211;page-requisites    下载显示HTML文件的所有图片递归下载中的包含和不包含(accept/reject)
-A,  &#8211;accept=LIST                分号分隔的被接受扩展名的列表
-R,  &#8211;reject=LIST                分号分隔的不被接受的扩展名的列表
-D,  &#8211;domains=LIST               分号分隔的被接受域的列表
&#8211;exclude-domains=LIST       分号分隔的不被接受的域的列表
&#8211;follow-ftp                 跟踪HTML文档中的FTP链接
&#8211;follow-tags=LIST           分号分隔的被跟踪的HTML标签的列表
-G,  &#8211;ignore-tags=LIST           分号分隔的被忽略的HTML标签的列表
-H,  &#8211;span-hosts                 当递归时转到外部主机
-L,  &#8211;relative                   仅仅跟踪相对链接
-I,  &#8211;include-directories=LIST   允许目录的列表
-X,  &#8211;exclude-directories=LIST   不被包含目录的列表
-np, &#8211;no-parent                  不要追溯到父目录</p>

<p>Wget使用技巧</p>

<p>来源：Linux技术中坚站</p>

<p>wget的使用形式是：
wget [参数列表] URL
首先来介绍一下wget的主要参数：
· -b：让wget在后台运行，记录文件写在当前目录下&#8221;wget-log&#8221;文件中；
· -t [nuber of times]：尝试次数，当wget无法与服务器建立连接时，尝试连接多少次
。比如&#8221;-t
120&#8221;表示尝试120次。当这一项为&#8221;0&#8221;的时候，指定尝试无穷多次直到连接成功为止，这个
设置非常有用，当对方服务器突然关机或者网络突然中断的时候，可以在恢复正常后继续
下载没有传完的文件；
· -c：断点续传，这也是个非常有用的设置，特别当下载比较大的文件的时候，如果中
途意外中断，那么连接恢复的时候会从上次没传完的地方接着传，而不是又从头开始，使
用这一项需要远程服务器也支持断点续传，一般来讲，基于UNIX/Linux的Web/FTP服务器
都支持断点续传；
· -T [number of seconds]：超时时间，指定多长时间远程服务器没有响应就中断连接
，开始下一次尝试。比如&#8221;-T
120&#8221;表示如果120秒以后远程服务器没有发过来数据，就重新尝试连接。如果网络速度比
较快，这个时间可以设置的短些，相反，可以设置的长一些，一般最多不超过900，通常
也不少于60，一般设置在120左右比较合适；
· -w [number of seconds]：在两次尝试之间等待多少秒，比如&#8221;-w 100&#8221;表示两次尝试
之间等待100秒；
· -Y on/off：通过／不通过代理服务器进行连接；
· -Q [byetes]：限制下载文件的总大小最多不能超过多少，比如&#8221;-Q2k&#8221;表示不能超过2K
字节，&#8221;-Q3m&#8221;表示最多不能超过3M字节，如果数字后面什么都不加，就表示是以字节为单
位，比如&#8221;-Q200&#8221;表示最多不能超过200字节；
· -nd：不下载目录结构，把从服务器所有指定目录下载的文件都堆到当前目录里；
· -x：与&#8221;-nd&#8221;设置刚好相反，创建完整的目录结构，例如&#8221;wget -nd
http://www.gnu.org&#8221;将创建在当前目录下创建&#8221;www.gnu.org&#8221;子目录，然后按照服务器
实际的目录结构一级一级建下去，直到所有的文件都传完为止；
· -nH：不创建以目标主机域名为目录名的目录，将目标主机的目录结构直接下到当前目
录下；
· &#8211;http-user=username
· &#8211;http-passwd=password：如果Web服务器需要指定用户名和口令，用这两项来设定；
· &#8211;proxy-user=username
· &#8211;proxy-passwd=password：如果代理服务器需要输入用户名和口令，使用这两个选项
；
· -r：在本机建立服务器端目录结构；
· -l [depth]：下载远程服务器目录结构的深度，例如&#8221;-l 5&#8221;下载目录深度小于或者等
于5以内的目录结构或者文件；
· -m：做站点镜像时的选项，如果你想做一个站点的镜像，使用这个选项，它将自动设
定其他合适的选项以便于站点镜像；
· -np：只下载目标站点指定目录及其子目录的内容。这也是一个非常有用的选项，我们
假设某个人的个人主页里面有一个指向这个站点其他人个人主页的连接，而我们只想下载
这个人的个人主页，如果不设置这个选项，甚至&#8211;有可能把整个站点给抓下来，这显然是
我们通常不希望的；
ü 如何设定wget所使用的代理服务器
wget可以使用用户设置文件&#8221;.wgetrc&#8221;来读取很多设置，我们这里主要利用这个文件来是
设置代理服务器。使用者用什么用户登录，那么什么用户主目录下的&#8221;.wgetrc&#8221;文件就起
作用。例如，&#8221;root&#8221;用户如果想使用&#8221;.wgetrc&#8221;来设置代理服务器，&#8221;/root/.wgert&#8221;就起
作用，下面给出一个&#8221;.wge
trc&#8221;文件的内容，读者可以参照这个例子来编写自己的&#8221;wgetrc&#8221;文件：
http-proxy = 111.111.111.111:8080
ftp-proxy = 111.111.111.111:8080
这两行的含义是，代理服务器IP地址为：111.111.111.111，端口号为：80。第一行指定
HTTP协议所使用的代理服务器，第二行指定FTP协议所使用的代理服务器。</p>

<p>wget 使用实例：
wget是一个命令行工具，用于批量下载文件，支持HTTP和FTP。究竟比其他的工具好在哪里？看看内容吧 :)</p>

<p>wget基本上所有的Linux版本都自己带了，但是有多少人在用呢？呵呵，如果你没有用过，不妨试试。Windows下面的用户可以使用GNUwin32的项目，wget，基本功能完全一致。好吧，我们来以几个简单的例子看看wget的威力。</p>

<p>如果我们想下载ftp里面某个目录里面的所有文件，我们也可以不用ftp这个笨蛋，呵呵，可以享受cute ftp等图形化工具的拖一个目录的轻松了。如</p>

<p>wget -r ftp://10.8.8.8/movie/</p>

<p>呵呵，等吧！下完了，发觉有些不对劲，怎么出来个10.8.8.8的目录，进去看看，又是一个movie，哦，wget将目录结构和网站标题都给记录下来了，不要？？没有问题！比如说还是这个例子</p>

<p>wget -r -nd ftp://10.8.8.8/movie/</p>

<p>结果什么目录都没有了，faint！怎么会这样？呵呵，你如果想要这样就让它这样吧，否则使用</p>

<p>wget -r -nH ftp://10.8.8.8/movie/</p>

<p>恩？movie也不要？OK，那就这样</p>

<p>wget -r -nH &#8211;cut-dirs=1 ftp://10.8.8.8/movie/</p>

<p>这有什么用啊？cuteftp比他好用多了，而且，你这断了线能连吗？呵呵，不好意思，可以连</p>

<p>wget -c -r -nH &#8211;cut-dirs=1 ftp://10.8.8.8/movie/</p>

<p>但 是cuteftp能做下面的事情吗？比如，现在很多网站使用Apache建站，并不提供ftp服务，但是Apache有一个indexing功能，可以提 供一个类似于ftp的界面，好多文件我想下啊，怎么办？由于是HTTP协议，CuteFTP无能为力了，倒是flash get等有什么get  all这种功能，不知道他们对于目录处理怎么样。但是wget一点问题都没有，不信？我们拿CTAN为例（例子并不恰当，CTAN有FTP服务），我们下 载这里面所有的宏包，呵呵</p>

<p>wget -r -k http://www.txia.com/blog</p>

<p>-k表示将连接转换为本地连接。但是现在同样有上面的问题啊，那就把你需要的加上吧，另外也许你根本不需要向下走那么多层，比如，我们就要到第二层，那么</p>

<p>wget -r -l2 -k http://www.txia.com/blog</p>

<p>现在新的问题是，由于网页有一个排序功能，很讨厌，因为下载的时候把网页重复了好多次，那么我们可使用-A和-R开关控制下载类型，并且可以使用通配符，呵呵，现在随心所欲了吧</p>

<p>wget -r -R &#8217;<em>.htm</em>\?*&#8217; -k http://www.txia.com/blog</p>

<p>这次没有那种网页了吧？-R的意义在于拒绝下载匹配类型的文件，-A表示仅仅接受的文件类型，如-A &#8216;*.gif&#8217;将仅下载gif图片，如果有多个允许或者不允许，可以使用,分开。</p>

<p>那 么，我们现在在使用代理服务器，怎么办呢？呵呵，很高兴你选择了wget，你可以使用它的配置文件，环境变量来利用代理。这里推荐使用环境变量，如在  bash里面我们可以把天天用的proxy加到.bash_profile里面，这是Linux标准写法（很多软件都用的，什么apt-get，yum等 等）</p>

<p>export http_proxy=http://10.20.30.40:8080</p>

<p>然后，proxy就默认打开了，如果需要暂时关闭，可以使用</p>

<p>wget &#8211;proxy=off -r -k http://www.txia.com/blog</p>

<p>当然，写一个.wgetrc文件也可以，该文件可以从/usr/local/etc里面找到，里面有很详细的注释，我就不多说了。</p>

<p>下载网页的时候比较麻烦的事情是，有的网页被同时指向了很多遍，那么为了避免多次下载，我们使用</p>

<p>wget -nc -r -k http://www.txia.com/blog</p>

<p>可以避免这件事情。为了不被有的连接指向非http://www.txia.com/blog内层目录，我们还应该加上</p>

<p>wget -nc -np -r -k http://www.txia.com/blog</p>

<p>避免下载非该目录里面的文件，这也避免了到不同的host上面去。当然，如果你希望有这个功能，在多个host之间跳来跳去的下载，可以使用</p>

<p>wget -nc -np -H -r -k http://www.txia.com/blog</p>

<p>使得可以在多个host之间span，同时-I和-X可以使得我们仅仅跟踪某些目录或者不跟踪某些目录。如果某些HTML里面你需要的东西不是由这种东西作出来的，你就得使用&#8211;follow-tags和&#8211;ignore-tags了。</p>

<p>嘿，我有个文件里面都是连接，怎么办？要是不是html你只需要</p>

<p>wget -i your.file</p>

<p>如果是，那也不繁</p>

<p>wget -F -i your.file</p>

<p>wget 使用指南
wget是一个从网络上自动下载文件的自由工具。它支持HTTP，HTTPS和FTP协议，可以使用HTTP代理.</p>

<p>所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。</p>

<p>wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作&#8221;递归下载&#8221;。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。</p>

<p>wget 非常稳定,它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务 器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。</p>

<p>wget的常见用法
wget的使用格式</p>

<p>Usage: wget [OPTION]&#8230; [URL]&#8230;</p>

<p>用wget做站点镜像:
wget -r -p -np -k http://dsec.pku.edu.cn/~us&#8230;
-r   表示递归下载,会下载所有的链接,不过要注意的是,不要单独使用这个参数,因为如果你要下载的网站也有别的网站的链接,wget也会把别的网站的东西下载 下来,所以要加上 -np这个参数,表示不下载别的站点的链接.  -k表示将下载的网页里的链接修改为本地链接.-p获得所有显示网页所需的元素,比如图片什么的.</p>

<h1>或者</h1>

<p>wget -m http://www.tldp.org/LDP/ab&#8230;</p>

<p>在不稳定的网络上下载一个部分下载的文件，以及在空闲时段下载
wget -t 0 -w 31 -c http://dsec.pku.edu.cn/BBC&#8230; -o down.log &amp;</p>

<h1>或者从filelist读入要下载的文件列表</h1>

<p>wget -t 0 -w 31 -c -B ftp://dsec.pku.edu.cn/linu&#8230; -i filelist.txt -o down.log &amp;</p>

<p>上面的代码还可以用来在网络比较空闲的时段进行下载。我的用法是:在mozilla中将不方便当时下载的URL链接拷贝到内存中然后粘贴到文件filelist.txt中，在晚上要出去系统前执行上面代码的第二条。</p>

<p>使用代理下载
wget -Y on -p -k https://sourceforge.net/pr&#8230;</p>

<p>代理可以在环境变量或wgetrc文件中设定</p>

<h1>在环境变量中设定代理</h1>

<p>export PROXY=http://211.90.168.94:8080/</p>

<h1>在~/.wgetrc中设定代理</h1>

<p>http_proxy = http://proxy.yoyodyne.com:&#8230;
ftp_proxy = http://proxy.yoyodyne.com:&#8230;</p>

<p>wget各种选项分类列表
启动
-V,  &#8211;version           显示wget的版本后退出
-h,  &#8211;help              打印语法帮助
-b,  &#8211;background        启动后转入后台执行
-e,  &#8211;execute=COMMAND   执行`.wgetrc&#8217;格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc</p>

<p>记录和输入文件
-o,  &#8211;output-file=FILE     把记录写到FILE文件中
-a,  &#8211;append-output=FILE   把记录追加到FILE文件中
-d,  &#8211;debug                打印调试输出
-q,  &#8211;quiet                安静模式(没有输出)
-v,  &#8211;verbose              冗长模式(这是缺省设置)
-nv, &#8211;non-verbose          关掉冗长模式，但不是安静模式
-i,  &#8211;input-file=FILE      下载在FILE文件中出现的URLs
-F,  &#8211;force-html           把输入文件当作HTML格式文件对待
-B,  &#8211;base=URL             将URL作为在-F -i参数指定的文件中出现的相对链接的前缀
&#8211;sslcertfile=FILE     可选客户端证书
&#8211;sslcertkey=KEYFILE   可选客户端证书的KEYFILE
&#8211;egd-file=FILE        指定EGD socket的文件名</p>

<p>下载
&#8211;bind-address=ADDRESS   指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)
-t,  &#8211;tries=NUMBER           设定最大尝试链接次数(0 表示无限制).
-O   &#8211;output-document=FILE   把文档写到FILE文件中
-nc, &#8211;no-clobber             不要覆盖存在的文件或使用.#前缀
-c,  &#8211;continue               接着下载没下载完的文件
&#8211;progress=TYPE          设定进程条标记
-N,  &#8211;timestamping           不要重新下载文件除非比本地文件新
-S,  &#8211;server-response        打印服务器的回应
&#8211;spider                 不下载任何东西
-T,  &#8211;timeout=SECONDS        设定响应超时的秒数
-w,  &#8211;wait=SECONDS           两次尝试之间间隔SECONDS秒
&#8211;waitretry=SECONDS      在重新链接之间等待1&#8230;SECONDS秒
&#8211;random-wait            在下载之间等待0&#8230;2*WAIT秒
-Y,  &#8211;proxy=on/off           打开或关闭代理
-Q,  &#8211;quota=NUMBER           设置下载的容量限制
&#8211;limit-rate=RATE        限定下载输率</p>

<p>目录
-nd  &#8211;no-directories            不创建目录
-x,  &#8211;force-directories         强制创建目录
-nH, &#8211;no-host-directories       不创建主机目录
-P,  &#8211;directory-prefix=PREFIX   将文件保存到目录 PREFIX/&#8230;
&#8211;cut-dirs=NUMBER           忽略 NUMBER层远程目录</p>

<p>HTTP 选项
&#8211;http-user=USER      设定HTTP用户名为 USER.
&#8211;http-passwd=PASS    设定http密码为 PASS.
-C,  &#8211;cache=on/off        允许/不允许服务器端的数据缓存 (一般情况下允许).
-E,  &#8211;html-extension      将所有text/html文档以.html扩展名保存
&#8211;ignore-length       忽略 <code>Content-Length'头域
--header=STRING       在headers中插入字符串 STRING
--proxy-user=USER     设定代理的用户名为 USER
--proxy-passwd=PASS   设定代理的密码为 PASS
--referer=URL         在HTTP请求中包含</code>Referer: URL&#8217;头
-s,  &#8211;save-headers        保存HTTP头到文件
-U,  &#8211;user-agent=AGENT    设定代理的名称为 AGENT而不是 Wget/VERSION.
&#8211;no-http-keep-alive  关闭 HTTP活动链接 (永远链接).
&#8211;cookies=off         不使用 cookies.
&#8211;load-cookies=FILE   在开始会话前从文件 FILE中加载cookie
&#8211;save-cookies=FILE   在会话结束后将 cookies保存到 FILE文件中</p>

<p>FTP 选项
-nr, &#8211;dont-remove-listing   不移走 `.listing&#8217;文件
-g,  &#8211;glob=on/off           打开或关闭文件名的 globbing机制
&#8211;passive-ftp           使用被动传输模式 (缺省值).
&#8211;active-ftp            使用主动传输模式
&#8211;retr-symlinks         在递归的时候，将链接指向文件(而不是目录)</p>

<p>递归下载
-r,  &#8211;recursive          递归下载－－慎用!
-l,  &#8211;level=NUMBER       最大递归深度 (inf 或 0 代表无穷).
&#8211;delete-after       在现在完毕后局部删除文件
-k,  &#8211;convert-links      转换非相对链接为相对链接
-K,  &#8211;backup-converted   在转换文件X之前，将之备份为 X.orig
-m,  &#8211;mirror             等价于 -r -N -l inf -nr.
-p,  &#8211;page-requisites    下载显示HTML文件的所有图片</p>

<p>递归下载中的包含和不包含(accept/reject)
-A,  &#8211;accept=LIST                分号分隔的被接受扩展名的列表
-R,  &#8211;reject=LIST                分号分隔的不被接受的扩展名的列表
-D,  &#8211;domains=LIST               分号分隔的被接受域的列表
&#8211;exclude-domains=LIST       分号分隔的不被接受的域的列表
&#8211;follow-ftp                 跟踪HTML文档中的FTP链接
&#8211;follow-tags=LIST           分号分隔的被跟踪的HTML标签的列表
-G,  &#8211;ignore-tags=LIST           分号分隔的被忽略的HTML标签的列表
-H,  &#8211;span-hosts                 当递归时转到外部主机
-L,  &#8211;relative                   仅仅跟踪相对链接
-I,  &#8211;include-directories=LIST   允许目录的列表
-X,  &#8211;exclude-directories=LIST   不被包含目录的列表
-np, &#8211;no-parent                  不要追溯到父目录</p>

<p>问题
在递归下载的时候，遇到目录中有中文的时候，wget创建的本地目录名会用URL编码规则处理。如&#8221;天网防火墙&#8221;会被存为&#8221;%CC%EC%CD%F8%B7%C0%BB%F0%C7%BD&#8221;,这造成阅读上的极大不方便</p>

<p>前几天用wget下了一些东西，发现一些很用的方法，记录一下。</p>

<p>用wget下载整个目录：</p>

<div id="highlighter_183963">
<div>
<div>
<table><tbody><tr>
<td></td>
<td>
<code>wget -t0 </code><code>-c</code> <code>-nH</code> <code>-np</code> <code>-b</code> <code>-m</code> <code>-P</code> <code>/localdir <a href="http://destinationdirectory/">http://destinationdirectory</a> </code><code>-o</code> <code>wget.log</code>
</td>
</tr></tbody></table>
</div>
</div>
</div>


<p>参数的说明如下：</p>

<ul>
<li>
<strong>-t number (&<a href="http://www.finalbug.org/tag/8211">8211</a>;tries=number)</strong> 重试次数，默认为20，设置成0或inf表示不限制重试次数。如果收到“connection refused”或“not found”（404）错误则不再重试。</li>
    <li>
<strong>-c （&<a href="http://www.finalbug.org/tag/8211">8211</a>;continue）</strong>续传</li>
    <li>
<strong>-nH （&<a href="http://www.finalbug.org/tag/8211">8211</a>;no-host-directories）</strong>禁止host前缀。默认情况下，执行“Wget -r http://xyz/”创建的目录结构将以“xyz”开始，该选项将禁止该功能。</li>
    <li>
<strong>-np（&<a href="http://www.finalbug.org/tag/8211">8211</a>;no-parent）</strong>在递归的时候忽略父级目录。</li>
    <li>
<strong>-b（&<a href="http://www.finalbug.org/tag/8211">8211</a>;background）</strong>启动之后在后台运行。通过-o可以设置记录文件，如果没有设置，默认将记录在wget-log文件里。</li>
    <li>
<strong>-m（&<a href="http://www.finalbug.org/tag/8211">8211</a>;mirror）</strong>寻找镜像。</li>
    <li>
<strong>-P prefix（&<a href="http://www.finalbug.org/tag/8211">8211</a>;directory-prefix=prefix）</strong>将“prefix”设置为下载目录的前缀。即将所有内容下载到prefix路径下。（注意，P大写）</li>
</ul>


<p>用wget递归下载整站：</p>

<div id="highlighter_145938">
<div>
<div>
<a title="view source" href="http://www.finalbug.org/2010/12/1753/#viewSource">view source</a><a title="print" href="http://www.finalbug.org/2010/12/1753/#printSource">print</a><a title="?" href="http://www.finalbug.org/2010/12/1753/#about">?</a>
</div>
</div>
<div>
<div>
<table><tbody><tr>
<td><code>1</code></td>
<td>
<code>wget </code><code>-r</code> <code>-p</code> <code>-np</code> <code>-k</code> <code><a href="http://xxx.com/abc/">http://xxx.com/abc/</a></code>
</td>
</tr></tbody></table>
</div>
</div>
</div>


<p>参数说明如下：</p>

<ul>
<li> <strong>-p（&<a href="http://www.finalbug.org/tag/8211">8211</a>;page-requisites）</strong>下载指定的HTML文件的所有相关内容，包括图片，声音，CSS文件。（注意，p小写）</li>
    <li>
<strong>-r（&<a href="http://www.finalbug.org/tag/8211">8211</a>;recursive）</strong>递归下载。</li>
    <li>
<strong>-k（&<a href="http://www.finalbug.org/tag/8211">8211</a>;convert-links）</strong>下载完成后，将下载文件中的链接转换成本地连接，包括图片路径，CSS等等。但是指向下载目录之外的链接将不会被修改。</li>
</ul>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-09T00:00:00+08:00" pubdate data-updated="true">Nov 9<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/linux/'>Linux</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/09/wget/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/08/social-networking/">
		
			社交网络</a>
	</h2>
	<div class="entry-content">
		<p>我的名字叫Mark Zuckerberg，我是一典型卷发犹太人，我成绩优秀，高中最爱编程，造了几个有点小用的软件后，我考上了哈佛. 在大学我什么都不缺，社交？那是上等社会有钱小孩的游戏，我羡慕但我不需要，女朋友？有一个我喜欢无比的女生Erica，但她没我聪明，从她去波士顿大学 就知道了. 我不会甜言蜜语，我向来直言直语，这就是为什么我女朋友会叫我*sshole然后和我分手. 我生气我愤怒，于是我在博客里说了她的坏话，我还专门因此建立一个叫走facemash的网站来表达我对所有女生的不屑. 我1小时内编程出来的网站在两小时内因为流量过大把哈佛的网络系统瘫痪，那又怎样? 我什么都不怕.</p>

<p>两个傻呵呵的兄弟俩找到我告诉我他们想建一个哈佛学生自己的社交网站，他们叫它The Harvard connection. 我觉得这主意不错，所以自己编了另一个网站叫做The facebook, 我和我的好朋友Eduardo Saverin一起合作，他出钱，我出力，我70%的股份他30%. 傻呵呵兄弟不爽了觉得我偷了他们的想法，但我觉得我没做错. 好朋友Eduardo Saverin也不爽了，因为我听信他人的流言蜚语最后把他的股份骗到0.03%而已, 他面对着我在律师的面前说，Mark Zuckerberg,我是你唯一的朋友. 而你背叛了我.</p>

<p>我无言，我坚信我不是坏人，但这一切为了什么. 最后我成为了全天下最年轻的亿万富翁，用仅仅几百万美元封住了这些人的嘴. 但我孤单的只剩下facebook,还有钱陪着我。</p>

<p>当故事结束，我做在我的电脑前，通过facebook找到了那个我曾经深深喜欢的女孩子, 我一遍遍看着她的头像, 犹犹豫豫的加了她为好友, 又一遍遍的刷新，期待着她能通过我的好友认证. 但最后, 我知道我其实什么等待的希望都没有了. 她又怎么知道，这个价值6.4 亿美元的网站只是我当时因为她离开我伤心难过而建造的呢？</p>

<p>[caption id=&#8221;attachment_62019&#8221; align=&#8221;aligncenter&#8221; width=&#8221;240&#8221; caption=&#8221;the facebook 4&#8221;]<a href="http://nwy.me/img/2010/11/p688238211.jpg"><img class="size-medium wp-image-62019 " title="the facebook 4" src="http://nwy.me/img/2010/11/p688238211-240x300.jpg" alt="" width="240" height="300"></a>[/caption]</p>

<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;我是分割线&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;</p>

<p>无意中被论坛的转贴剧透，David Fincher最新的作品,导演过<fight club> <seven> 其中<fight club>看来不下三遍，这部影片从获知一直很期待，本地影院还没有上映，于是打算等网络上高清出来再看，结果现在还没有看电影结局都知道了，sigh，之前有朋友开玩笑威胁我要剧透这部影片的剧情，被我以格式化他笔记本硬盘震慑住了，没想到最后还是被剧透了，世事难料阿。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-08T00:00:00+08:00" pubdate data-updated="true">Nov 8<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/personal/'>Personal</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/08/social-networking/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/08/other/">
		
			晚睡原来是一种病——拖延症 (Procrastination) Procrastination的形成</a>
	</h2>
	<div class="entry-content">
		<div>
<strong> </strong>1． 一个人认为自己5天之内可以做完一件事情，所以在离deadline还有15天的时候一点不着急，直到最后只剩5天了才开始。

2． 这种紧迫感和焦虑往往促发人的斗志，会让自己觉得，自己只有在压力状态下才有做事情的状态。

3． 最后拿到成绩的时候，成绩往往不是很差，这样子就强化了自己最适合在deadline之前短期高压的状态下工作的心态，并且对以后的行为不断进行自我暗示。

这一个部分写得非常符合大部分有拖沓习惯的中国学生的经历。因为中国学生往往非常聪明，所以哪怕最后只剩一点点时间了，也会完成得不错；从而自认为自己最适合这样子的工作状态。周而复始，反复循环。

<strong>Procrastination的其他特点 </strong>

1． 没有自信。因为每次完成任务都达不到自己最高的能力，对自我能力的评估会越来越低。
2． 我太忙。我一直拖着没做因为我一直很忙。
3． 顽固。你催我也没有用。我准备好了自然会开始做。
4． 操控别人。他们着急也没用，一切都要等我到了才能开始。
5． 对抗压力。因为每天压力很大，所以要做的事情一直被拖下来。
6． 受害者心态。我也知道自己怎么会这样，别人能做得自己做不到。

<strong>Procrastination的浅层原因 </strong>

<strong>
		
		<a href="/blog/2010/11/08/other/" class="more-link">Read on &rarr;</a>
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-08T00:00:00+08:00" pubdate data-updated="true">Nov 8<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/other/'>Other</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/08/other/#disqus_thread">Comments</a></div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2010/11/07/batch-change-the-extension-for-linux/">
		
			Linux下批量修改扩展名</a>
	</h2>
	<div class="entry-content">
		<p>这两天在整理一些照片，在Linux下文件名区分大小写，而要处理的照片的扩展名都是JPG导致在软件和上传页面里不能识别，需要将JPG改成jpg，于是发挥电工的智慧通过正则命令批量修改扩展名。</p>

<p>下面演示将/home/ocean/2010/下JPG文件批量修改为jpg文件.如sam_4533.JPG修改为sam_4533.jpg</p>

<ol>
<li><p><code lang="bash">for file in <em>.JPG;do mv $file ${file%.</em>}.jpg;done</code></p></li>
<li><p><code lang="bash">for i in *.JPG; do mv $i <code>basename $i .JPG</code>.jpg ;done</code></p></li>
<li><p><code lang="bash">rename 'y/A-Z/a-z/' *  </code>
rename在不同的发行版是不同，此处的是perl-rename，还有c语言的rename</p></li>
</ol>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2010-11-07T00:00:00+08:00" pubdate data-updated="true">Nov 7<span>th</span>, 2010</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/linux/'>Linux</a>


</div>
	
	<div class="comments"><a href="/blog/2010/11/07/batch-change-the-extension-for-linux/#disqus_thread">Comments</a></div>
	
</div>
</article>

<nav id="pagenavi">
    
        <a href="/blog/page/6/" class="prev">Prev</a>
    
    
        <a href="/blog/page/8/" class="next">Next</a>
    
    <div class="center"><a href="/blog/archives">Blog Archives</a></div>
</nav></div>
	<footer id="footer" class="inner">Copyright &copy; 2013

    Ocean

<div style="display:none"> <script language="javascript" type="text/javascript" src="http://js.users.51.la/4388679.js"></script>
<noscript><a href="http://www.51.la/?4388679" target="_blank"><img alt="&#x6211;&#x8981;&#x5566;&#x514D;&#x8D39;&#x7EDF;&#x8BA1;" src="http://img.users.51.la/4388679.asp" style="border:none" /></a></noscript> </div></footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->


<script type="text/javascript">
      var disqus_shortname = 'oceanblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//go.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





</body>
</html>